{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 20:19:37.285022: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-09 20:19:37.285089: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from PIL import Image\n",
    "# import os\n",
    "# from tensorflow.python.keras.models load_model\n",
    "# from keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "# import matplotlib.pyplot as plt\n",
    "# from keras.preprocessing import image\n",
    "# import numpy as np\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 20:19:44.845675: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-09 20:19:44.845733: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-09 20:19:44.845760: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nidhiK-singh): /proc/driver/nvidia/version does not exist\n",
      "2022-05-09 20:19:44.846164: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Building the CNN\n",
    "\n",
    "# Initializing the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.add(Convolution2D(32, (3, 3), input_shape=(64, 64, 1), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), input_shape=(64, 64, 1), activation='relu'))\n",
    "# classifier.add(Convolution2D(32, (3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Second convolution layer and pooling\n",
    "classifier.add(Convolution2D(32, (3, 3), activation='relu'))\n",
    "# input_shape is going to be the pooled feature maps from the previous convolution layer\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flattening the layers\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Adding a fully connected layer\n",
    "classifier.add(Dense(units=200, activation='relu'))\n",
    "classifier.add(Dense(units=28, activation='softmax')) # softmax for more than 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # categorical_crossentropy for more than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310, 310, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv2.imread(\"/home/nidhik/signlanguagedirectory/code/homepetry/data1/try/4.jpg\").shape\n",
    "# cv2.imread(\"/home/nidhik/signlanguagedirectory/code/homepetry/data1/try/0.jpg\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6757 images belonging to 28 classes.\n",
      "Found 336 images belonging to 28 classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 2 - Preparing the train/test data and training the model\n",
    "\n",
    "# Code copied from - https://keras.io/preprocessing/image/\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        )\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('/home/nidhik/signlanguagedirectory/code/data2/train',\n",
    "                                                 target_size=(64, 64),\n",
    "                                                 batch_size=5,\n",
    "                                                 color_mode='grayscale',\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('/home/nidhik/signlanguagedirectory/code/data2/test',\n",
    "                                            target_size=(64, 64),\n",
    "                                            batch_size=5,\n",
    "                                            color_mode='grayscale',\n",
    "                                            class_mode='categorical') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1352/1352 [==============================] - 508s 374ms/step - loss: 0.8687 - accuracy: 0.7542 - val_loss: 0.4887 - val_accuracy: 0.8333\n",
      "Epoch 2/10\n",
      "1352/1352 [==============================] - 432s 320ms/step - loss: 0.1876 - accuracy: 0.9461 - val_loss: 0.5497 - val_accuracy: 0.9167\n",
      "Epoch 3/10\n",
      "1352/1352 [==============================] - 79s 58ms/step - loss: 0.1041 - accuracy: 0.9688 - val_loss: 0.5181 - val_accuracy: 0.9167\n",
      "Epoch 4/10\n",
      "1352/1352 [==============================] - 70s 52ms/step - loss: 0.0726 - accuracy: 0.9800 - val_loss: 0.0263 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1352/1352 [==============================] - 68s 51ms/step - loss: 0.0578 - accuracy: 0.9831 - val_loss: 0.1742 - val_accuracy: 0.9667\n",
      "Epoch 6/10\n",
      "1352/1352 [==============================] - 68s 50ms/step - loss: 0.0500 - accuracy: 0.9858 - val_loss: 0.1423 - val_accuracy: 0.9500\n",
      "Epoch 7/10\n",
      "1352/1352 [==============================] - 61s 45ms/step - loss: 0.0429 - accuracy: 0.9880 - val_loss: 0.5839 - val_accuracy: 0.9167\n",
      "Epoch 8/10\n",
      "1352/1352 [==============================] - 62s 46ms/step - loss: 0.0358 - accuracy: 0.9885 - val_loss: 0.0641 - val_accuracy: 0.9833\n",
      "Epoch 9/10\n",
      "1352/1352 [==============================] - 68s 50ms/step - loss: 0.0311 - accuracy: 0.9916 - val_loss: 0.1986 - val_accuracy: 0.9667\n",
      "Epoch 10/10\n",
      "1352/1352 [==============================] - 67s 49ms/step - loss: 0.0282 - accuracy: 0.9914 - val_loss: 0.2473 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81bc4b48b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(\n",
    "        training_set,\n",
    "        # steps_per_epoch=6757, # No of images in training set\n",
    "        epochs=10,\n",
    "        batch_size=5,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=12)# No of images in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               1254600   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 28)                5628      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,269,796\n",
      "Trainable params: 1,269,796\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.fit(\n",
    "#         training_set,\n",
    "#         # steps_per_epoch=700, # No of images in training set\n",
    "#         epochs=20,\n",
    "#         batch_size=5,\n",
    "#         validation_data=test_set,\n",
    "#         validation_steps=50)# No of images in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img=load_img(\"/home/nidhik/signlanguagedirectory/code/homepetry/data1/train/6/6.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_path=\"/home/nidhik/signlanguagedirectory/code/homepetry/data1/try\"\n",
    "# for i in os.listdir(dir_path):\n",
    "#   img = load_img(dir_path + \"//\" + i)\n",
    "  \n",
    "#   plt.imshow(img)\n",
    "#   plt.show()\n",
    "#   img = img.resize((64, 64,1), Image.ANTIALIAS)\n",
    "\n",
    "#   x=image.img_to_array(img)\n",
    "#   print(x.shape)\n",
    "\n",
    "#   x=x/255\n",
    "\n",
    "#   x=np.expand_dims(x,axis=0)\n",
    "#   images=np.vstack([x])\n",
    "#   val=classifier.predict(images)\n",
    "# if val=='1':\n",
    "#      print(\"one\")\n",
    "# elif val=='6':\n",
    "#    print(\"six\")\n",
    "\n",
    "# else:\n",
    "#      print(\"blahhh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "# def get_config(self):\n",
    "\n",
    "#         config = super().get_config().copy()\n",
    "#         config.update({\n",
    "#             'vocab_size': self.vocab_size,\n",
    "#             'num_layers': self.num_layers,\n",
    "#             'units': self.units,\n",
    "#             'd_model': self.d_model,\n",
    "#             'num_heads': self.num_heads,\n",
    "#             'dropout': self.dropout,\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "\n",
    "# Saving the model\n",
    "model_json = classifier.to_json()\n",
    "with open(\"model-bw.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "classifier.save_weights('model-bw.h5')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04ed55e9766531cf231e9cc649bbc8855f9882a7edcbd181a173f84c35a081a9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('my_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
